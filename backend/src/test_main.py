from transformers import AutoTokenizer

from main import get_segments


# @TODO: the random emoji test does not work:
# "ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ ğŸ‘©â€ğŸ‘§â€ğŸ‘§ ğŸ‘©â€ğŸ‘©â€ğŸ‘¦ ğŸ‘©â€ğŸ‘©â€ğŸ‘§ ğŸ‡¨ğŸ‡¿" -> ["ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦", " ğŸ‘©â€ğŸ‘§â€ğŸ‘¦", " ğŸ‘©â€ğŸ‘§â€ğŸ‘§", " ğŸ‘©â€ğŸ‘©â€ğŸ‘¦", " ğŸ‘©â€ğŸ‘©â€ğŸ‘§", " ğŸ‡¨ğŸ‡¿"]
def test_filter_step_data():
    tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")

    assert [x.text for x in get_segments(tokenizer, "ç”±éç‡Ÿåˆ©çµ„ç¹”")] == ["ç”±", "é", "ç‡Ÿ", "åˆ©", "çµ„", "ç¹”"]
